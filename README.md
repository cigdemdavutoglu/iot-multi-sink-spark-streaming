# IoT Telemetry Streaming with Spark Structured Streaming

Bu proje, IoT cihazlarÄ±ndan gelen sensÃ¶r verilerini gerÃ§ek zamanlÄ± olarak okuyup, belirli kurallara gÃ¶re **Ã¼Ã§ farklÄ± hedefe** yazan bir **Spark Structured Streaming** uygulamasÄ±dÄ±r.

## ğŸ“Œ Projenin AmacÄ±

GerÃ§ek zamanlÄ± veri iÅŸleme ile:
- `device` ID'sine gÃ¶re verileri filtrelemek,
- Filtrelenen verileri farklÄ± hedeflere yÃ¶nlendirmek:
  - ğŸ” **PostgreSQL** (device: `00:0f:00:70:91:0a`)
  - ğŸ **Hive (Parquet)** (device: `b8:27:eb:bf:9d:51`)
  - ğŸŒ€ **Delta Lake (HDFS)** (device: `1c:bf:ce:15:ec:4d`)
 
  ## ğŸ“‚ Veri ve Tablo OluÅŸturma

 **Ã–NEMLÄ°:** Bu proje iÃ§in Spark ve Hadoop ortamÄ±nÄ±zÄ±n doÄŸru ÅŸekilde kurulmuÅŸ ve yapÄ±landÄ±rÄ±lmÄ±ÅŸ olmasÄ± gerekmektedir.

-KullanÄ±lan IoT sensÃ¶r verileri `iot_telemetry_data.csv.zip` dosyasÄ±nda bulunur.

-Sanal makinenin terminalinde output dosyasÄ± oluÅŸturulur ve bu dosyanÄ±n bulunduÄŸu klasÃ¶rÃ¼n dizimine gelindikten sonra venvspark (source venvspark/bin/activate) aktif edildilir.

-python dataframe_to_log.py -i ~/datasets/iot_telemetry_data.csv -idx True (Bu komut, verileri output klasÃ¶rÃ¼ne yazÄ±p streamin veri almasÄ±nÄ± saÄŸlar.) komutuyla Ã§alÄ±ÅŸtÄ±rÄ±lÄ±r.

--Sink oluÅŸturma:
- Hive tablosu, DBeaver kullanÄ±larak manuel oluÅŸturulabilir.
-   CREATE TABLE test1.sensor_51 (
    row_id INT,
    event_ts DOUBLE,
    device STRING,
    co FLOAT,
    humidity FLOAT,
    light BOOLEAN,
    lpg FLOAT,
    motion BOOLEAN,
    smoke FLOAT,
    temp FLOAT,
    generate_ts TIMESTAMP
)
STORED AS PARQUET;
  Tablo yapÄ±sÄ± ve adÄ±: `test1.sensor_51`  
  (Gerekirse Hive terminal veya DBeaver Ã¼zerinden oluÅŸturulabilir.)

- PostgreSQL tablosu streaming sÄ±rasÄ±nda otomatik oluÅŸturulmaktadÄ±r.

- Delta tablosu, `create_delta_table.py` dosyasÄ± ile HDFS Ã¼zerinde oluÅŸturulmuÅŸtur.

- read_from_csv.py dosyasÄ± Ã§alÄ±ÅŸtÄ±rÄ±lÄ±r.(Bu dosya veriyi gerÃ§ek zamanlÄ± olarak okur,veriyi device IDâ€™sine gÃ¶re filtreler,filtrelenen verileri Ã¼Ã§ farklÄ± hedefe (PostgreSQL, Hive, Delta Lake) yazmak iÃ§in kullanÄ±lÄ±r.)
- Ä°lgili verilerin sink'lerdeki tablolara gidip gitmediÄŸini gÃ¶rmek iÃ§in sorgulama yapÄ±lÄ±r.
- Delta table iÃ§in read_delta_table.py dosyasÄ± Ã§alÄ±ÅŸtÄ±rÄ±larak gelen veriler gÃ¶zlenlenir.
- Postgresql ve hive iÃ§in dbeaver'da baÄŸlantÄ± saÄŸlanÄ±p select sorgusu atÄ±larak gÃ¶zlemlenebilir.


## ğŸ§± Proje YapÄ±sÄ±

- `read_from_csv.py`       : Spark Structured Streaming uygulamasÄ±,
- `create_delta_table.py`  : Delta tablosunu HDFS Ã¼zerinde oluÅŸturan script,
- `read_delta_table.py`    : Delta tablosundaki verileri okuyan script,
- `iot_telemetry_data.csv.zip` : IoT sensÃ¶r veri seti,
- `requirements.txt`       : Projede kullanÄ±lan Python paketleri,
- `README.md`              : Proje aÃ§Ä±klamasÄ± ve kullanÄ±m rehberi,
- `LICENSE`                : Proje lisansÄ±,
- `.gitignore`             : Git tarafÄ±ndan yok sayÄ±lacak dosya/klasÃ¶rler.


## âš™ï¸ KullanÄ±m

1. Projeyi klonlayÄ±n veya indirin:
```bash
git clone <repo-url>
cd <repo-folder>

## âš™ï¸ Gerekli Python paketlerini yÃ¼kleyin
pip install -r requirements.txt
spark-submit read_from_csv.py



